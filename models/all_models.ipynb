{"cells":[{"cell_type":"code","execution_count":null,"id":"031d798d-c6f1-4210-ac31-50a645cdb08c","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3641,"status":"ok","timestamp":1726143299448,"user":{"displayName":"Rasmus Klitte Andersen","userId":"00812341989558572059"},"user_tz":-120},"id":"031d798d-c6f1-4210-ac31-50a645cdb08c","outputId":"d651387e-35de-4e34-f412-7babe22b2f55","tags":[]},"outputs":[],"source":["#!pip install tensorflow==2.15.0  # Erstat med den ønskede version. Denne er nødvendig for at bruge TCN\n","#!pip install keras_tuner\n","#!pip install keras-tcn\n","# Grundlæggende pakker\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# Deep Learning med TensorFlow og Keras\n","import tensorflow as tf\n","from tensorflow.keras.utils import plot_model\n","\n","import logging\n","import os\n","import sys\n","import importlib\n","\n","# Check if GPU is available and configure memory growth\n","physical_devices = tf.config.list_physical_devices('GPU')\n","if physical_devices:\n","    try:\n","        for gpu in physical_devices:\n","            tf.config.experimental.set_memory_growth(gpu, True)\n","        print(\"GPU is available and memory growth enabled\")\n","    except RuntimeError as e:\n","        print(\"Memory growth setting failed:\", e)\n","else:\n","    print(\"No GPU found. Using CPU.\")\n","\n","# Check TensorFlow version for reference\n","print(\"TensorFlow Version:\", tf.__version__)\n","\n","# Additional GPU details\n","gpus = tf.config.list_physical_devices('GPU')\n","print(\"Available GPUs:\", gpus)\n","\n","# Optional: To specify a particular GPU, if multiple are available\n","gpu_number = 0  # Change this index if needed\n","if len(gpus) >= 2:\n","    try:\n","        tf.config.experimental.set_visible_devices(gpus[gpu_number], 'GPU')\n","        print(f\"Using GPU: {gpus[gpu_number]}\")\n","    except RuntimeError as e:\n","        print(\"Failed to set specific GPU:\", e)\n","\n","ROOT_PATH = '/Users/rasmusklitteandersen/Library/CloudStorage/GoogleDrive-rasmusklitteandersen@gmail.com/Mit drev/speciale/'\n","\n","# ROOT_PATH = '/content/drive/MyDrive/speciale/'\n","# from google.colab import drive\n","# drive.mount('/content/drive')\n","\n","# gpu_info = !nvidia-smi\n","# gpu_info = '\\n'.join(gpu_info)\n","# if gpu_info.find('failed') >= 0:\n","#   print('Not connected to a GPU')\n","# else:\n","#   print(gpu_info)\n","\n","# from psutil import virtual_memory\n","# ram_gb = virtual_memory().total / 1e9\n","# print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n","\n","# if ram_gb < 20:\n","#   print('Not using a high-RAM runtime')\n","# else:\n","#   print('You are using a high-RAM runtime!')"]},{"cell_type":"code","execution_count":null,"id":"3c6ce2ad","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3134,"status":"ok","timestamp":1726143319984,"user":{"displayName":"Rasmus Klitte Andersen","userId":"00812341989558572059"},"user_tz":-120},"id":"3c6ce2ad","outputId":"73a47ae1-b72a-43c6-ba9a-ad832bcfa2a7"},"outputs":[],"source":["# Set up paths and configurations\n","#ROOT_PATH = '/content/drive/MyDrive/speciale/'\n","sys.path.append(ROOT_PATH)\n","os.chdir(ROOT_PATH)\n","\n","# Import custom utilities and modules\n","from utils.misc import LoadData, TerminateNaN, Plotting, EvaluationMetric\n","from utils.models import ModelTrainer\n","\n","# ===== CONFIGURATION SECTION =====\n","# Paths\n","DATA_PATH = f'{ROOT_PATH}data/final_dataset_test.csv'\n","MODEL_PATH = f'{ROOT_PATH}models/'\n","IMAGE_PATH = f'{ROOT_PATH}/images/'\n","LOG_DIR = f'{MODEL_PATH}/logs/'\n","TUNING_DIR = f'{MODEL_PATH}/tuning/'\n","RESULTS_DIR = f'{ROOT_PATH}results/'\n","TABLES_DIR = f'{ROOT_PATH}tables/'\n","\n","# Flags and Options\n","INCLUDE_LAGS = True\n","INCLUDE_SEASON_VARS = True\n","INCLUDE_WEATHER = True\n","TEST = False\n","TIME_START = '2019-10-31'\n","TIME_END_PERIODS = ['2021-09-30', '2023-01-01', '2024-07-01']\n","MODELS = ['LSTM', 'TCN', 'Hybrid', 'Transformer']\n","#MODELS = ['Hybrid', 'Transformer']\n","TUNER = 'Hyperband'\n","\n","# Training Parameters\n","TRAINING_EPOCH = 30\n","FINAL_MODEL_EPOCH = 50\n","BATCH_SIZE = 64\n","LOSS = 'mean_absolute_error'\n","\n","# Generate dynamic strings based on flags\n","def get_extra_info():\n","    extra_info = ''\n","    if not INCLUDE_WEATHER:\n","        extra_info += '_no_weather'\n","    if not INCLUDE_LAGS:\n","        extra_info += '_no_lags'\n","    if not INCLUDE_SEASON_VARS:\n","        extra_info += '_no_season_vars'\n","    return extra_info\n","\n","# ===== LOGGING SETUP =====\n","logging.basicConfig(level=logging.INFO)\n","logger = logging.getLogger()\n","\n","# ===== FUNCTIONS =====\n","\n","def reload_utils_misc():\n","    \"\"\"Function to reload utils.misc module if changes are made.\"\"\"\n","    import utils.misc\n","    importlib.reload(utils.misc)\n","\n","\n","def setup_directories(test_mode, image_path, log_dir, tuning_dir, results_dir, tables_dir):\n","    \"\"\"Set up directories based on whether TEST mode is enabled.\"\"\"\n","    test_suffix = 'test/'\n","    \n","    if test_mode: \n","        image_path = f\"{image_path}{test_suffix}\"\n","        log_dir = f\"{log_dir}{test_suffix}\"\n","        tuning_dir = f\"{tuning_dir}{test_suffix}\"\n","        results_dir = f\"{results_dir}{test_suffix}\"\n","        tables_dir = f\"{tables_dir}{test_suffix}\"\n","        \n","    return image_path, log_dir, tuning_dir, results_dir, tables_dir\n","\n","\n","def initialize_data_loader():\n","    \"\"\"Initialize the data loading utility with required flags.\"\"\"\n","    return LoadData(INCLUDE_LAGS, INCLUDE_SEASON_VARS, INCLUDE_WEATHER, TUNER)\n","\n","def initialize_evaluation_metric():\n","    \"\"\"Initialize the evaluation metric utility with required flags.\"\"\"\n","    return EvaluationMetric(INCLUDE_LAGS, INCLUDE_SEASON_VARS, INCLUDE_WEATHER, TUNER)\n","\n","# ===== MAIN SCRIPT =====\n","\n","if __name__ == \"__main__\":\n","    # Setup directories based on TEST mode\n","    # dirs = setup_directories(TEST)\n","    IMAGE_PATH, LOG_DIR, TUNING_DIR, RESULTS_DIR, TABLES_DIR = setup_directories(\n","    test_mode=TEST, \n","    image_path=IMAGE_PATH, \n","    log_dir=LOG_DIR, \n","    tuning_dir=TUNING_DIR, \n","    results_dir=RESULTS_DIR, \n","    tables_dir=TABLES_DIR\n","    )\n","\n","    extra_info = get_extra_info()\n","    \n","    # Reload utils if changes were made\n","    reload_utils_misc()\n","\n","    # Initialize components\n","    load_data = initialize_data_loader()\n","    terminate_nan = TerminateNaN()\n","    evaluation_metric = initialize_evaluation_metric()\n","\n","    # Set random seeds for reproducibility\n","    tf.random.set_seed(14)\n","    np.random.seed(14)\n","\n","    # Log basic configuration\n","    logger.info(f\"Starting training with configuration:\")\n","    logger.info(f\"Data path: {DATA_PATH}\")\n","    logger.info(f\"Model path: {MODEL_PATH}\")\n","    logger.info(f\"Model path: {TUNING_DIR}\")\n","    logger.info(f\"Include Lags: {INCLUDE_LAGS}, Include Season Vars: {INCLUDE_SEASON_VARS}, Include Weather: {INCLUDE_WEATHER}\")\n","    logger.info(f\"Training epochs: {TRAINING_EPOCH}, Final model epochs: {FINAL_MODEL_EPOCH}, Batch size: {BATCH_SIZE}\")"]},{"cell_type":"markdown","id":"f013e083","metadata":{"id":"f013e083"},"source":["# All models"]},{"cell_type":"markdown","id":"99b2534c","metadata":{},"source":["## New setup"]},{"cell_type":"code","execution_count":null,"id":"c4b450c0","metadata":{},"outputs":[],"source":["#MODELS = ['LSTM','TCN', 'Hybrid', 'Transformer']\n","\n","for i in range(len(MODELS)):\n","    MODEL = MODELS[i]\n","    print(f'Model: {MODEL}')\n","    for i in range(len(TIME_END_PERIODS)):\n","        \n","        # Set the seed for reproducibility\n","        tf.random.set_seed(14)\n","        np.random.seed(14)\n","\n","        TIME_END = TIME_END_PERIODS[i]\n","        df, TIME_PERIOD = load_data.load_and_preprocess_data(DATA_PATH, TIME_START, TIME_END)\n","        print(f'Time period: {TIME_PERIOD}')\n","        log_dir, tuning_dir = load_data.setup_directories(LOG_DIR, TUNING_DIR, TIME_PERIOD, MODEL)\n","        datasets, y, X_train, X_test, scaler_y, y_test = load_data.split_and_scale_data(df)\n","        \n","        sequences_dict, scalers, X_test_scaled = load_data.prepare_sequences(df)\n","        datasets, y, X_train, X_test, scaler_y, y_test = load_data.split_and_scale_data(df)\n","        \n","        # Initialize and train the model\n","        trainer = ModelTrainer(datasets, log_dir, tuning_dir, LOSS, model_type=MODEL, sequences_dict=sequences_dict)\n","        final_model, history, best_hps, duration = trainer.train_model(BATCH_SIZE, TRAINING_EPOCH, FINAL_MODEL_EPOCH, overwrite=False)\n","        \n","        # Predictions and evaluation\n","        predictions = evaluation_metric.predict_full_sequence(final_model, X_test_scaled, scaler_y)\n","        if len(predictions) > len(y_test):\n","            predictions = predictions[:len(y_test)]\n","        else:\n","            y_test = y_test[:len(predictions)]\n","            \n","        evaluation_metric.evaluate_and_log(y_test, predictions, TIME_PERIOD, RESULTS_DIR, best_hps, duration, MODEL)\n","        \n","        # Plotting results\n","        plotting = Plotting(MODEL, TIME_PERIOD, IMAGE_PATH, INCLUDE_LAGS, INCLUDE_SEASON_VARS, INCLUDE_WEATHER, TUNER)\n","\n","        plotting.plot_predictions(df, y, X_train, X_test, np.array(predictions[:len(y_test)]))\n","        plotting.plot_training_history(history)\n","        plot_model(final_model, to_file=f'{IMAGE_PATH}/{MODEL}/{MODEL}_model{extra_info}_{TIME_PERIOD}.png', show_shapes=True, show_layer_names=True)"]},{"cell_type":"markdown","id":"9a53d868","metadata":{},"source":["## Single model"]},{"cell_type":"code","execution_count":null,"id":"0bc7738b","metadata":{},"outputs":[],"source":["MODEL = 'Transformer'\n","print(f'Model: {MODEL}')\n","\n","for i in range(len(TIME_END_PERIODS)):\n","    # Set the seed for reproducibility\n","    tf.random.set_seed(14)\n","    np.random.seed(14)\n","\n","    TIME_END = TIME_END_PERIODS[i]\n","    df, TIME_PERIOD = load_data.load_and_preprocess_data(DATA_PATH, TIME_START, TIME_END)\n","    print(f'Time period: {TIME_PERIOD}')\n","    log_dir, tuning_dir = load_data.setup_directories(LOG_DIR, TUNING_DIR, TIME_PERIOD, MODEL)\n","    sequences_dict, scalers, X_test_scaled = load_data.prepare_sequences(df)\n","    datasets, y, X_train, X_test, scaler_y, y_test = load_data.split_and_scale_data(df)\n","    \n","    # Initialize and train the model\n","    trainer = ModelTrainer(datasets, log_dir, tuning_dir, LOSS, model_type=MODEL, sequences_dict=sequences_dict)\n","    final_model, history, best_hps, duration = trainer.train_model(BATCH_SIZE, training_epoch=TRAINING_EPOCH, \n","                                                                   final_model_epoch=FINAL_MODEL_EPOCH, overwrite=False)\n","    \n","    # Predictions and evaluation\n","    predictions = evaluation_metric.predict_full_sequence(final_model, X_test_scaled, scaler_y)\n","    if len(predictions) > len(y_test):\n","        predictions = predictions[:len(y_test)]\n","    else:\n","        y_test = y_test[:len(predictions)]\n","        \n","    evaluation_metric.evaluate_and_log(y_test, predictions, TIME_PERIOD, RESULTS_DIR, best_hps, duration, MODEL)\n","    \n","    # Plotting results\n","    plotting = Plotting(MODEL, TIME_PERIOD, IMAGE_PATH, INCLUDE_LAGS, INCLUDE_SEASON_VARS, INCLUDE_WEATHER, TUNER)\n","\n","    plotting.plot_predictions(df, y, X_train, X_test, np.array(predictions[:len(y_test)]))\n","    plotting.plot_training_history(history)\n","    plot_model(final_model, to_file=f'{IMAGE_PATH}{MODEL}/{MODEL}_model{extra_info}_{TIME_PERIOD}.png', show_shapes=True, show_layer_names=True)"]},{"cell_type":"markdown","id":"c782e1ea","metadata":{},"source":["## SHAP Value"]},{"cell_type":"code","execution_count":null,"id":"d185761d","metadata":{},"outputs":[],"source":["TIME_END = TIME_END_PERIODS[2]\n","print(f'Time end: {TIME_END}')\n","MODEL = 'LSTM'\n","df, TIME_PERIOD = load_data.load_and_preprocess_data(DATA_PATH, TIME_START, TIME_END)\n","print(f'Time period: {TIME_PERIOD}')\n","log_dir, tuning_dir = load_data.setup_directories(LOG_DIR, TUNING_DIR, TIME_PERIOD, MODEL)\n","\n","#datasets, y, X_train, X_test, scaler_y, y_test = load_data.split_and_scale_data(df)\n","sequences_dict, datasets, scalers, X_test_scaled, X = load_data.prepare_sequences(df)\n","\n","# Initialize and train the model\n","trainer = ModelTrainer(datasets, log_dir, tuning_dir, LOSS, model_type=MODEL, sequences_dict=sequences_dict)\n","final_model, history, best_hps, duration = trainer.train_model(BATCH_SIZE, training_epoch=1, \n","                                                               final_model_epoch=1, overwrite=False)\n"]},{"cell_type":"code","execution_count":null,"id":"a74360a1","metadata":{},"outputs":[],"source":["import shap\n","import pandas as pd\n","# Assuming sequences_dict[\"test\"][\"X\"][1] is intended to be converted into a DataFrame\n","# Assuming sequences_dict[\"test\"][\"X\"][1] is intended to be converted into a DataFrame\n","X_test = sequences_dict[\"test\"][\"X\"][1]\n","\n","# Ensure X_test is a DataFrame\n","if isinstance(X_test, np.ndarray):\n","    # If it's a NumPy array, convert it into a DataFrame.\n","    X_test = pd.DataFrame(X_test, columns=[f\"Feature {i}\" for i in range(X_test.shape[1])])\n","\n","# Function to reshape and predict using the final model\n","def model_predict(data):\n","    # Reshape the input for prediction (expected: num_samples, 1, 18)\n","    data_reshaped = data.reshape((-1, 1, X_test.shape[1]))  # Now using the correct number of features\n","    return final_model(data_reshaped).numpy()  # Make prediction\n","\n","# Create the SHAP KernelExplainer object with the original data\n","explainer = shap.KernelExplainer(model_predict, X_test[:100])  # Using a small background dataset\n","\n","# Reshape for predictions – keep it as (10, number_of_features)\n","reshaped_for_prediction = X_test[:10]  # Using the first 10 instances\n","\n","# Calculate SHAP values for the first 10 instances\n","shap_values = explainer.shap_values(reshaped_for_prediction)  # Shape is (10, number_of_features)\n","\n","# Visualize SHAP values for the first instance\n","shap.initjs()\n","\n","# Manually create an Explanation object to include feature names\n","expl = shap.Explanation(\n","    values=shap_values[0],                  # SHAP values for the first instance\n","    feature_names=X_test.columns.tolist()    # Pass feature names from the DataFrame\n",")\n","\n","# Create a bar plot to summarize SHAP values for the first instance\n","shap.plots.bar(expl)"]},{"cell_type":"code","execution_count":null,"id":"bb5cb080","metadata":{},"outputs":[],"source":["data = np.squeeze(sequences_dict[\"test\"][\"X\"][:10])\n","\n","data_reshaped = data.reshape((-1, 1, X_test.shape[1])) \n","print(pd.DataFrame(data_reshaped).shape)"]},{"cell_type":"code","execution_count":null,"id":"cf889874","metadata":{},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import shap\n","\n","# Assuming sequences_dict[\"test\"][\"X\"][1] should be a DataFrame, convert to DataFrame if necessary\n","X_test = sequences_dict[\"test\"][\"X\"][:10]\n","\n","# Ensure X_test is a DataFrame\n","if isinstance(X_test, np.ndarray):\n","    # If it's a NumPy array, you can create a DataFrame with dummy column names or your own naming.\n","    X_test = pd.DataFrame(X_test)\n","\n","def model_predict(data):\n","    # Reshape the input for prediction (expected: num_samples, 1, 18)\n","    data_reshaped = np.squeeze(data).reshape((-1, 1, X_test.shape[1]))  # Reshape to (num_samples, 1, 18)\n","    return final_model(data_reshaped).numpy()  # Make prediction\n","\n","# Create the SHAP KernelExplainer object with the original data\n","explainer = shap.KernelExplainer(model_predict, X_test[:100])  # Using the original data for background\n","\n","# Reshape for predictions – Keep it as (10, 18)\n","reshaped_for_prediction = X_test[:10]  # Using the first 10 instances\n","\n","# Calculate SHAP values for the first 10 instances\n","shap_values = explainer.shap_values(reshaped_for_prediction)  # Shape now (10, 18)\n","\n","# Visualize SHAP values for the first instance\n","shap.initjs()\n","\n"," # Manually create an Explanation object to include feature names\n","expl = shap.Explanation(\n","        values=shap_values[0],                  # SHAP values from the model\n","        feature_names=X_test.columns.tolist(),  # Pass feature names\n","        data=reshaped_for_prediction            # Original data for SHAP\n","    )\n","\n","# Create a bar plot to summarize SHAP values for the first instance\n","shap.plots.bar(expl, max_display=len(X_test.columns))"]},{"cell_type":"code","execution_count":null,"id":"0f632278","metadata":{},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import shap\n","\n","# Create your augmented dataset from sequences_dict\n","X_test = sequences_dict[\"test\"][\"X\"][:10]  # Get the first 100 samples from your augmented dataset\n","feature_names = sequences_dict[\"test\"][\"X\"][1].columns\n","\n","print(f\"Original X_test type: {type(X_test)}, length: {len(X_test)}\")  # Show the type and length of the list\n","\n","# Ensure X_test is an array-like structure that can be processed\n","if isinstance(X_test, list):\n","    # Validate the structure of the input; print the first element's structure\n","    print(f\"First element structure: {type(X_test[0])}, shape: {np.shape(X_test[0]) if isinstance(X_test[0], np.ndarray) else 'N/A'}\")\n","\n","    # Assuming X_test is a list of 3D arrays or lists\n","    # Check if nested list structure\n","    try:\n","        # Convert to NumPy array for easier manipulation\n","        X_test = np.array(X_test)  # Shape: (100, 48, 18)\n","    except Exception as e:\n","        print(\"Error converting X_test to array:\", e)\n","        raise\n","\n","if isinstance(X_test, np.ndarray):\n","    # Check the dimensions of the array\n","    if X_test.ndim == 3:\n","        # Averaging across the time steps to reduce to (100, 18)\n","        # Here, decide how you want to handle the time steps\n","        X_test = np.mean(X_test, axis=1)  # Taking the mean across the time steps\n","        # Convert into a DataFrame with features\n","        X_test = pd.DataFrame(X_test, columns=feature_names)\n","    elif X_test.ndim == 2:  # If it's of shape (100, 18)\n","        # Directly convert to DataFrame\n","        X_test = pd.DataFrame(X_test, columns=feature_names)\n","\n","# Function to reshape and predict using the final model\n","def model_predict(data):\n","    # Reshape the input for prediction (expected: num_samples, 1, num_features)\n","    data_reshaped = data.reshape((-1, 1, X_test.shape[1]))  # Reshape to (num_samples, 1, num_features)\n","    return final_model(data_reshaped).numpy()  # Make prediction\n","\n","# Create the SHAP KernelExplainer object with the modified dataset\n","explainer = shap.Explainer(model_predict, X_test.values)\n","\n","# Use the entire dataset for computation\n","reshaped_for_prediction = X_test.values  # The modified dataset for SHAP values\n","\n","# Calculate SHAP values for the modified dataset\n","shap_values = explainer.shap_values(reshaped_for_prediction)\n","\n","# Visualize SHAP values for the first instance\n","shap.initjs()\n","mean_abs_shap_values = np.mean(np.abs(shap_values), axis=0)\n","\n","\n","# Create a SHAP Explanation object for the first instance using actual column names\n","expl = shap.Explanation(\n","    values=mean_abs_shap_values\n",")\n","\n","# Create a bar plot to summarize SHAP values for the first instance\n","shap.plots.bar(expl)\n","\n"]},{"cell_type":"code","execution_count":134,"id":"91aff106","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/envs/speciale-env/lib/python3.12/site-packages/keras/src/models/functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: keras_tensor_7. Received: the structure of inputs=('*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*')\n","  warnings.warn(\n"]},{"ename":"ValueError","evalue":"Exception encountered when calling Sequential.call().\n\n\u001b[1mInvalid input shape for input Tensor(\"data:0\", shape=(32,), dtype=float32). Expected shape (None, 48, 18), but input has incompatible shape (32,)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=('tf.Tensor(shape=(32,), dtype=float32)', 'tf.Tensor(shape=(32,), dtype=float32)', 'tf.Tensor(shape=(32,), dtype=float32)', 'tf.Tensor(shape=(32,), dtype=float32)', 'tf.Tensor(shape=(32,), dtype=float32)', 'tf.Tensor(shape=(32,), dtype=float32)', 'tf.Tensor(shape=(32,), dtype=float32)', 'tf.Tensor(shape=(32,), dtype=float32)', 'tf.Tensor(shape=(32,), dtype=float32)', 'tf.Tensor(shape=(32,), dtype=float32)', 'tf.Tensor(shape=(32,), dtype=float32)', 'tf.Tensor(shape=(32,), dtype=float32)', 'tf.Tensor(shape=(32,), dtype=float32)', 'tf.Tensor(shape=(32,), dtype=float32)', 'tf.Tensor(shape=(32,), dtype=float32)', 'tf.Tensor(shape=(32,), dtype=float32)', 'tf.Tensor(shape=(32,), dtype=float32)', 'tf.Tensor(shape=(32,), dtype=float32)')\n  • training=False\n  • mask=('None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None')","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[134], line 37\u001b[0m\n\u001b[1;32m     34\u001b[0m explainer \u001b[38;5;241m=\u001b[39m shap\u001b[38;5;241m.\u001b[39mExplainer(f, X_test\u001b[38;5;241m.\u001b[39mvalues)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Calculate SHAP values for the modified dataset\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m shap_values \u001b[38;5;241m=\u001b[39m \u001b[43mexplainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshap_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Visualize SHAP values for the first instance\u001b[39;00m\n\u001b[1;32m     40\u001b[0m shap\u001b[38;5;241m.\u001b[39minitjs()\n","File \u001b[0;32m/opt/anaconda3/envs/speciale-env/lib/python3.12/site-packages/shap/explainers/_permutation.py:208\u001b[0m, in \u001b[0;36mPermutationExplainer.shap_values\u001b[0;34m(self, X, npermutations, main_effects, error_bounds, batch_evals, silent)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshap_values\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, npermutations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, main_effects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, error_bounds\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, batch_evals\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, silent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    184\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Legacy interface to estimate the SHAP values for a set of samples.\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \n\u001b[1;32m    186\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    206\u001b[0m \n\u001b[1;32m    207\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     explanation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnpermutations\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmain_effects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmain_effects\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m explanation\u001b[38;5;241m.\u001b[39mvalues\n","File \u001b[0;32m/opt/anaconda3/envs/speciale-env/lib/python3.12/site-packages/shap/explainers/_permutation.py:77\u001b[0m, in \u001b[0;36mPermutationExplainer.__call__\u001b[0;34m(self, max_evals, main_effects, error_bounds, batch_size, outputs, silent, *args)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, max_evals\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m, main_effects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, error_bounds\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     75\u001b[0m              outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, silent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     76\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Explain the output of the model on the given arguments.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_evals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmain_effects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmain_effects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_bounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_bounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msilent\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/anaconda3/envs/speciale-env/lib/python3.12/site-packages/shap/explainers/_explainer.py:266\u001b[0m, in \u001b[0;36mExplainer.__call__\u001b[0;34m(self, max_evals, main_effects, error_bounds, batch_size, outputs, silent, *args, **kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m     feature_names \u001b[38;5;241m=\u001b[39m [[] \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(args))]\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row_args \u001b[38;5;129;01min\u001b[39;00m show_progress(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39margs), num_rows, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m explainer\u001b[39m\u001b[38;5;124m\"\u001b[39m, silent):\n\u001b[0;32m--> 266\u001b[0m     row_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexplain_row\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrow_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_evals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmain_effects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmain_effects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_bounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_bounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msilent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    270\u001b[0m     values\u001b[38;5;241m.\u001b[39mappend(row_result\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    271\u001b[0m     output_indices\u001b[38;5;241m.\u001b[39mappend(row_result\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_indices\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n","File \u001b[0;32m/opt/anaconda3/envs/speciale-env/lib/python3.12/site-packages/shap/explainers/_permutation.py:133\u001b[0m, in \u001b[0;36mPermutationExplainer.explain_row\u001b[0;34m(self, max_evals, main_effects, error_bounds, batch_size, outputs, silent, *row_args)\u001b[0m\n\u001b[1;32m    130\u001b[0m     i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m# evaluate the masked model\u001b[39;00m\n\u001b[0;32m--> 133\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mfm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzero_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m row_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    136\u001b[0m     row_values \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mlen\u001b[39m(fm),) \u001b[38;5;241m+\u001b[39m outputs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:])\n","File \u001b[0;32m/opt/anaconda3/envs/speciale-env/lib/python3.12/site-packages/shap/utils/_masked_model.py:60\u001b[0m, in \u001b[0;36mMaskedModel.__call__\u001b[0;34m(self, masks, zero_index, batch_size)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(masks\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmasker, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msupports_delta_masking\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m---> 60\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_delta_masking_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzero_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mzero_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;66;03m# we need to convert from delta masking to a full masking call because we were given a delta masking\u001b[39;00m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;66;03m# input but the masker does not support delta masking\u001b[39;00m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m         full_masks \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mint\u001b[39m(np\u001b[38;5;241m.\u001b[39msum(masks \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_masker_cols), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m)\n","File \u001b[0;32m/opt/anaconda3/envs/speciale-env/lib/python3.12/site-packages/shap/utils/_masked_model.py:206\u001b[0m, in \u001b[0;36mMaskedModel._delta_masking_call\u001b[0;34m(self, masks, zero_index, batch_size)\u001b[0m\n\u001b[1;32m    203\u001b[0m     batch_positions[i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m batch_positions[i] \u001b[38;5;241m+\u001b[39m num_varying_rows[i]\n\u001b[1;32m    205\u001b[0m \u001b[38;5;66;03m# joined_masked_inputs = self._stack_inputs(all_masked_inputs)\u001b[39;00m\n\u001b[0;32m--> 206\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msubset_masked_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m _assert_output_input_match(subset_masked_inputs, outputs)\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinearize_link \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlink \u001b[38;5;241m!=\u001b[39m links\u001b[38;5;241m.\u001b[39midentity \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_linearizing_weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","File \u001b[0;32m/opt/anaconda3/envs/speciale-env/lib/python3.12/site-packages/shap/models/_model.py:21\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m---> 21\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minner_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     is_tensor \u001b[38;5;241m=\u001b[39m safe_isinstance(out, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.Tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m     out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;28;01mif\u001b[39;00m is_tensor \u001b[38;5;28;01melse\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(out)\n","Cell \u001b[0;32mIn[134], line 31\u001b[0m, in \u001b[0;36mf\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mf\u001b[39m(X):\n\u001b[0;32m---> 31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfinal_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mflatten()\n","File \u001b[0;32m/opt/anaconda3/envs/speciale-env/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n","File \u001b[0;32m/opt/anaconda3/envs/speciale-env/lib/python3.12/site-packages/keras/src/models/functional.py:264\u001b[0m, in \u001b[0;36mFunctional._adjust_input_rank\u001b[0;34m(self, flat_inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m             adjusted\u001b[38;5;241m.\u001b[39mappend(ops\u001b[38;5;241m.\u001b[39mexpand_dims(x, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    263\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 264\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    265\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid input shape for input \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Expected shape \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    266\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mref_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but input has incompatible shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    267\u001b[0m     )\n\u001b[1;32m    268\u001b[0m \u001b[38;5;66;03m# Add back metadata.\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(flat_inputs)):\n","\u001b[0;31mValueError\u001b[0m: Exception encountered when calling Sequential.call().\n\n\u001b[1mInvalid input shape for input Tensor(\"data:0\", shape=(32,), dtype=float32). Expected shape (None, 48, 18), but input has incompatible shape (32,)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=('tf.Tensor(shape=(32,), dtype=float32)', 'tf.Tensor(shape=(32,), dtype=float32)', 'tf.Tensor(shape=(32,), dtype=float32)', 'tf.Tensor(shape=(32,), dtype=float32)', 'tf.Tensor(shape=(32,), dtype=float32)', 'tf.Tensor(shape=(32,), dtype=float32)', 'tf.Tensor(shape=(32,), dtype=float32)', 'tf.Tensor(shape=(32,), dtype=float32)', 'tf.Tensor(shape=(32,), dtype=float32)', 'tf.Tensor(shape=(32,), dtype=float32)', 'tf.Tensor(shape=(32,), dtype=float32)', 'tf.Tensor(shape=(32,), dtype=float32)', 'tf.Tensor(shape=(32,), dtype=float32)', 'tf.Tensor(shape=(32,), dtype=float32)', 'tf.Tensor(shape=(32,), dtype=float32)', 'tf.Tensor(shape=(32,), dtype=float32)', 'tf.Tensor(shape=(32,), dtype=float32)', 'tf.Tensor(shape=(32,), dtype=float32)')\n  • training=False\n  • mask=('None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None')"]}],"source":["import numpy as np\n","import pandas as pd\n","import shap\n","\n","# Create your augmented dataset from sequences_dict\n","X_test = sequences_dict[\"test\"][\"X\"][:10]  # Get the first 10 samples\n","\n","# Extract feature names\n","feature_names = sequences_dict[\"test\"][\"X\"][1].columns.tolist()  # Ensure these are correct\n","\n","# Ensure X_test is an array-like structure that can be processed\n","if isinstance(X_test, list):\n","    # Convert to NumPy array if it is a list of lists or arrays\n","    X_test = np.array(X_test)  # Shape should be (10, 48, 18)\n","\n","if isinstance(X_test, np.ndarray):\n","    if X_test.ndim == 3:\n","        # Averaging across time steps to reduce to (10, 18)\n","        X_test = np.mean(X_test, axis=1)  # Shape will now be (10, 18)\n","    \n","    # Convert into a DataFrame with features\n","    X_test = pd.DataFrame(X_test, columns=feature_names[:X_test.shape[1]])  # Match to the number of columns\n","\n","# Function to reshape and predict using the final model\n","def model_predict(data):\n","    # Reshape the input for prediction (expected: num_samples, 1, num_features)\n","    data_reshaped = data.reshape((-1, 1, X_test.shape[1]))  # Reshape to (num_samples, 1, num_features)\n","    return final_model(data_reshaped).numpy()  # Make prediction\n","\n","def f(X):\n","    return final_model.predict([X[:, i] for i in range(X.shape[1])]).flatten()\n","\n","# Create the SHAP Explainer object with the modified dataset\n","explainer = shap.Explainer(model_predict, X_test.values)\n","\n","# Calculate SHAP values for the modified dataset\n","shap_values = explainer.shap_values(X_test.values)\n","\n","# Visualize SHAP values for the first instance\n","shap.initjs()\n","\n","# Compute mean absolute SHAP values across all instances\n","mean_abs_shap_values = np.mean(np.abs(shap_values), axis=0)\n","\n","# Create a SHAP Explanation object including the feature names\n","expl = shap.Explanation(\n","    values=mean_abs_shap_values,           # Mean absolute SHAP values\n","    feature_names=feature_names  # Pass feature names for proper labeling\n",")\n","\n","# Create a bar plot to summarize SHAP values\n","shap.plots.bar(expl)"]},{"cell_type":"code","execution_count":null,"id":"43380812","metadata":{},"outputs":[],"source":["#Visualize SHAP values for LSTM\n","shap.initjs()\n","\n","# Create figure and axis\n","fig, ax = plt.subplots()\n","\n","shap.plots.bar(shap_values_with_names,show=False, ax=ax, max_display=len(shap_values_with_names))\n","plt.savefig(f'{IMAGE_PATH}{MODEL}/{MODEL}_shap_bar_full_dataset.png', bbox_inches='tight')  # Save as PNG (you can choose other formats like PDF)\n","plt.close()\n","shap.plots.beeswarm(shap_values_with_names, max_display=len(shap_values_with_names), show=False)\n","plt.savefig(f'{IMAGE_PATH}{MODEL}/{MODEL}_shap_beeswarm_full_dataset.png', bbox_inches='tight')  # Save as PNG (you can choose other formats like PDF)\n","plt.close()\n","shap.plots.waterfall(shap_values_with_names[0],show=False)\n","plt.savefig(f'{IMAGE_PATH}{MODEL}/{MODEL}_shap_waterfall_full_dataset.png', bbox_inches='tight')  # Save as PNG (you can choose other formats like PDF)\n","plt.close()\n","\n","# Force plot (for the first instance)\n","force_plot = shap.plots.force(shap_values_with_names[0])\n","\n","# Save force plot as HTML\n","shap.save_html(f'{IMAGE_PATH}{MODEL}/{MODEL}_shap_force_plot_full_dataset.html', force_plot)"]},{"cell_type":"code","execution_count":null,"id":"5a8353bc","metadata":{},"outputs":[],"source":["MODEL = MODELS[1]\n","TIME_END = TIME_END_PERIODS[1]\n","df, TIME_PERIOD = load_data.load_and_preprocess_data(DATA_PATH, TIME_START, TIME_END)\n","print(f'Time period: {TIME_PERIOD}')\n","log_dir, tuning_dir = load_data.setup_directories(LOG_DIR, TUNING_DIR, TIME_PERIOD, MODEL)\n","datasets, y, X_train, X_test, scaler_y, y_test = load_data.split_and_scale_data(df)\n","\n","\n","print(len(X_test.columns))"]},{"cell_type":"code","execution_count":null,"id":"8d2afde5","metadata":{},"outputs":[],"source":["import shap\n","\n","for i in range(len(MODELS)):\n","    MODEL = MODELS[i]\n","    print(f'Model: {MODEL}')\n","    for i in range(len(TIME_END_PERIODS)):\n","        # Set the seed for reproducibility\n","        tf.random.set_seed(14)\n","        np.random.seed(14)\n","\n","        TIME_END = TIME_END_PERIODS[i]\n","        df, TIME_PERIOD = load_data.load_and_preprocess_data(DATA_PATH, TIME_START, TIME_END)\n","        print(f'Time period: {TIME_PERIOD}')\n","        log_dir, tuning_dir = load_data.setup_directories(LOG_DIR, TUNING_DIR, TIME_PERIOD, MODEL)\n","        datasets, y, X_train, X_test, scaler_y, y_test = load_data.split_and_scale_data(df)\n","        \n","        # Initialize and train the model\n","        trainer = ModelTrainer(datasets, log_dir, tuning_dir, LOSS, model_type=MODEL)\n","        final_model, history, best_hps, duration = trainer.train_model(BATCH_SIZE, TRAINING_EPOCH, FINAL_MODEL_EPOCH, overwrite=False)\n","\n","        # SHAP values\n","        sample_size = 100\n","        X_sample = np.squeeze(datasets['X_train'][:sample_size])  #[:sample_size] #X_sample = X_train[:sample_size] #datasets['X_train'][:sample_size]\n","        X_test_sample = np.squeeze(datasets['X_test'][:sample_size])  #[:sample_size] #X_val_sample = X_test[:10] #datasets['X_val'][:sample_size]\n","        feature_names=X_test.columns\n","\n","        explainer = shap.Explainer(final_model, X_sample)\n","        shap_values = explainer(X_test_sample)\n","\n","        # Manually create an Explanation object to include feature names\n","        shap_values_with_names = shap.Explanation(\n","            values=shap_values,       # SHAP values from the model\n","            feature_names=feature_names, # Pass feature names\n","            data=X_test_sample          # Original data for SHAP\n","        )\n","        \n","        #Visualize SHAP values for LSTM\n","        shap.initjs()\n","\n","        # Create figure and axis\n","        fig, ax = plt.subplots()\n","\n","        shap.plots.bar(shap_values_with_names,show=False, ax=ax, max_display=len(shap_values_with_names))\n","        plt.savefig(f'{IMAGE_PATH}{MODEL}/{MODEL}_shap_bar_{TIME_PERIOD}_{extra_info}_sample_{sample_size}.png', bbox_inches='tight')  # Save as PNG (you can choose other formats like PDF)\n","        plt.close()\n","        shap.plots.beeswarm(shap_values_with_names, max_display=len(shap_values_with_names), show=False)\n","        plt.savefig(f'{IMAGE_PATH}{MODEL}/{MODEL}_shap_beeswarm_{TIME_PERIOD}_{extra_info}_sample_{sample_size}.png', bbox_inches='tight')  # Save as PNG (you can choose other formats like PDF)\n","        plt.close()\n","        shap.plots.waterfall(shap_values_with_names[0],show=False)\n","        plt.savefig(f'{IMAGE_PATH}{MODEL}/{MODEL}_shap_waterfall_{TIME_PERIOD}_{extra_info}_sample_{sample_size}.png', bbox_inches='tight')  # Save as PNG (you can choose other formats like PDF)\n","        plt.close()\n","\n","        # Force plot (for the first instance)\n","        force_plot = shap.plots.force(shap_values_with_names[0])\n","\n","        # Save force plot as HTML\n","        shap.save_html(f'{IMAGE_PATH}{MODEL}/{MODEL}_shap_force_plot_{TIME_PERIOD}_{extra_info}_sample_{sample_size}.html', force_plot)"]},{"cell_type":"code","execution_count":null,"id":"7a8698e5","metadata":{},"outputs":[],"source":["import shap\n","from tqdm import tqdm\n","\n","# Wrap the outer loop with tqdm for model progress\n","for i in tqdm(range(len(MODELS)), desc=\"Processing Models\"):\n","    MODEL = MODELS[i]\n","    print(f'Model: {MODEL}')\n","    \n","    # Wrap the inner loop with tqdm for time period progress\n","    for i in tqdm(range(len(TIME_END_PERIODS)), desc=\"Processing Time Periods\", leave=False):\n","        # Set the seed for reproducibility\n","        tf.random.set_seed(14)\n","        np.random.seed(14)\n","\n","        TIME_END = TIME_END_PERIODS[i]\n","        df, TIME_PERIOD = load_data.load_and_preprocess_data(DATA_PATH, TIME_START, TIME_END)\n","        print(f'Time period: {TIME_PERIOD}')\n","        log_dir, tuning_dir = load_data.setup_directories(LOG_DIR, TUNING_DIR, TIME_PERIOD, MODEL)\n","        datasets, y, X_train, X_test, scaler_y, y_test = load_data.split_and_scale_data(df)\n","        \n","        # Initialize and train the model\n","        trainer = ModelTrainer(datasets, log_dir, tuning_dir, LOSS, model_type=MODEL)\n","        final_model, history, best_hps, duration = trainer.train_model(BATCH_SIZE, TRAINING_EPOCH, FINAL_MODEL_EPOCH, overwrite=False)\n","\n","        # SHAP values\n","        sample_size = 100\n","        X_sample = np.squeeze(datasets['X_train'][:sample_size])\n","        X_test_sample = np.squeeze(datasets['X_test'][:sample_size])\n","        feature_names = X_test.columns\n","\n","        explainer = shap.Explainer(final_model, X_sample)\n","        shap_values = explainer(X_test_sample)\n","\n","        # Manually create an Explanation object to include feature names\n","        shap_values_with_names = shap.Explanation(\n","            values=shap_values,       # SHAP values from the model\n","            feature_names=feature_names, # Pass feature names\n","            data=X_test_sample          # Original data for SHAP\n","        )\n","        \n","        # Visualize SHAP values for LSTM\n","        shap.initjs()\n","\n","        # Create figure and axis\n","        fig, ax = plt.subplots()\n","        shap.plots.bar(shap_values_with_names, show=False, ax=ax, max_display=len(shap_values_with_names))\n","        plt.savefig(f'{IMAGE_PATH}{MODEL}/{MODEL}_shap_bar_{TIME_PERIOD}_{extra_info}_sample_{sample_size}.png', bbox_inches='tight')\n","        plt.close()\n","        \n","        shap.plots.beeswarm(shap_values_with_names, max_display=len(shap_values_with_names), show=False)\n","        plt.savefig(f'{IMAGE_PATH}{MODEL}/{MODEL}_shap_beeswarm_{TIME_PERIOD}_{extra_info}_sample_{sample_size}.png', bbox_inches='tight')\n","        plt.close()\n","        \n","        shap.plots.waterfall(shap_values_with_names[0], show=False)\n","        plt.savefig(f'{IMAGE_PATH}{MODEL}/{MODEL}_shap_waterfall_{TIME_PERIOD}_{extra_info}_sample_{sample_size}.png', bbox_inches='tight')\n","        plt.close()\n","\n","        # Force plot (for the first instance)\n","        force_plot = shap.plots.force(shap_values_with_names[0])\n","\n","        # Save force plot as HTML\n","        shap.save_html(f'{IMAGE_PATH}{MODEL}/{MODEL}_shap_force_plot_{TIME_PERIOD}_{extra_info}_sample_{sample_size}.html', force_plot)\n"]},{"cell_type":"code","execution_count":null,"id":"8558e779","metadata":{},"outputs":[],"source":["#Visualize SHAP values for LSTM\n","shap.initjs()\n","\n","# Create figure and axis\n","fig, ax = plt.subplots()\n","\n","shap.plots.bar(shap_values_with_names, ax=ax, max_display=len(shap_values_with_names))\n","shap.plots.beeswarm(shap_values_with_names, max_display=len(shap_values_with_names))\n","shap.plots.waterfall(shap_values_with_names[0], max_display=len(shap_values_with_names))\n","shap.plots.force(shap_values_with_names[0])"]},{"cell_type":"markdown","id":"f8292af4","metadata":{},"source":["# Diebold-Mariano"]},{"cell_type":"code","execution_count":null,"id":"6bdc5fcf","metadata":{},"outputs":[],"source":["import pickle\n","\n","\n","TIME_END = TIME_END_PERIODS[2]\n","print(f'Time end: {TIME_END}')\n","MODEL = 'Hybrid'\n","df, TIME_PERIOD = load_data.load_and_preprocess_data(DATA_PATH, TIME_START, TIME_END)\n","print(f'Time period: {TIME_PERIOD}')\n","\n","with open(f'/Users/rasmusklitteandersen/Library/CloudStorage/GoogleDrive-rasmusklitteandersen@gmail.com/Mit drev/speciale/results/test/predictions/{MODEL}/{MODEL}_{TIME_PERIOD}_Hyperband_predictions.pkl', 'rb') as f:\n","    predictions_1 = pickle.load(f)\n","\n","\n","MODEL = 'Transformer'\n","with open(f'/Users/rasmusklitteandersen/Library/CloudStorage/GoogleDrive-rasmusklitteandersen@gmail.com/Mit drev/speciale/results/test/predictions/{MODEL}/{MODEL}_{TIME_PERIOD}_Hyperband_predictions.pkl', 'rb') as f:\n","    predictions_2 = pickle.load(f)\n","\n","\n","\n","log_dir, tuning_dir = load_data.setup_directories(LOG_DIR, TUNING_DIR, TIME_PERIOD, MODEL)\n","datasets, y, X_train, X_test, scaler_y, y_test = load_data.split_and_scale_data(df)\n"]},{"cell_type":"code","execution_count":null,"id":"6a3195a0","metadata":{},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from statsmodels.tsa.stattools import acf\n","from scipy import stats\n","\n","#mae = metrics.mae(np.array(y_test[:len(predictions)]), np.array(predictions))\n","\n","errors_model1 = np.abs(np.array(y_test[:len(predictions)]) - predictions_1)\n","errors_model2 = np.abs(np.array(y_test[:len(predictions)]) - predictions_2)\n","d = errors_model1 - errors_model2\n","print(d)\n","\n","# Sample function for Diebold-Mariano test\n","def diebold_mariano_test(errors_model1, errors_model2):\n","    d = errors_model1 - errors_model2\n","    d = np.ravel(d)\n","    mean_d = np.mean(d)\n","    acf_values = acf(d, fft=True)\n","    variance_d = np.var(d) * (1 + 2 * sum(acf_values[1:])) / len(d)\n","    dm_stat = mean_d / np.sqrt(variance_d)\n","    p_value = 2 * (1 - stats.norm.cdf(abs(dm_stat)))\n","    return dm_stat, p_value\n","\n","# Perform Diebold-Mariano test on the errors\n","dm_stat, p_value = diebold_mariano_test(errors_model1, errors_model2)\n","print(f\"Diebold-Mariano Test Statistic: {dm_stat}\")\n","print(f\"P-Value: {p_value}\")\n","\n","# Interpretation\n","if p_value < 0.05:\n","    print(\"The difference in forecasting errors is statistically significant.\")\n","else:\n","    print(\"The difference in forecasting errors is not statistically significant.\")\n","\n"]},{"cell_type":"markdown","id":"1e12dd82","metadata":{},"source":["# Old"]},{"cell_type":"code","execution_count":null,"id":"60617672","metadata":{},"outputs":[],"source":["\n","for i in range(len(MODELS)):\n","    MODEL = MODELS[i]\n","    print(f'Model: {MODEL}')\n","    for i in range(len(TIME_END_PERIODS)):\n","        # Set the seed for reproducibility\n","        tf.random.set_seed(14)\n","        np.random.seed(14)\n","\n","        TIME_END = TIME_END_PERIODS[i]\n","        df, TIME_PERIOD = load_data.load_and_preprocess_data(DATA_PATH, TIME_START, TIME_END)\n","        print(f'Time period: {TIME_PERIOD}')\n","        log_dir, tuning_dir = load_data.setup_directories(LOG_DIR, TUNING_DIR, TIME_PERIOD, MODEL)\n","        datasets, y, X_train, X_test, scaler_y, y_test = load_data.split_and_scale_data(df)\n","        \n","        # Initialize and train the model\n","        trainer = ModelTrainer(datasets, log_dir, tuning_dir, LOSS, model_type=MODEL)\n","        final_model, history, best_hps, duration = trainer.train_model(BATCH_SIZE, TRAINING_EPOCH, FINAL_MODEL_EPOCH, overwrite=False)\n","        \n","        # Predictions and evaluation\n","        predictions = evaluation_metric.predict_future_values(datasets, scaler_y, final_model, n_hours_ahead=24)\n","        evaluation_metric.evaluate_and_log(y_test, predictions, TIME_PERIOD, RESULTS_DIR, best_hps, duration, MODEL)\n","        \n","        # Plotting results\n","        plotting = Plotting(MODEL, TIME_PERIOD, IMAGE_PATH, INCLUDE_LAGS, INCLUDE_SEASON_VARS, INCLUDE_WEATHER, TUNER)\n","        plotting.plot_predictions(df, y, X_train, X_test, predictions)\n","        plotting.plot_training_history(history)\n","        plot_model(final_model, to_file=f'{IMAGE_PATH}/{MODEL}/{MODEL}_model{extra_info}_{TIME_PERIOD}.png', show_shapes=True, show_layer_names=True)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"}},"nbformat":4,"nbformat_minor":5}
