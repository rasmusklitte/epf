{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "431dff92-af36-42eb-8245-b555218c40a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests # library for making HTTP requests\n",
    "import pandas as pd # library for data analysis\n",
    "import datetime as dt # library for handling date and time objects\n",
    "import matplotlib.pyplot as plt # library for creating plots\n",
    "\n",
    "path_to_images = ''\n",
    "root_path = ''\n",
    "data_path = f'{root_path}data/final_dataset.csv'\n",
    "model_path = f'{root_path}models/'\n",
    "path_to_images = f'{root_path}images/'\n",
    "base_log_dir = f'{model_path}logs/'\n",
    "base_tuning_dir = f'{model_path}tuning/'\n",
    "results_dir = f'{root_path}results/'\n",
    "tables_dir = f'{root_path}tables/'\n",
    "\n",
    "# Specify the desired start and end time\n",
    "start_time = pd.Timestamp(2019, 10, 31)\n",
    "end_time = pd.Timestamp(2024, 7, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196988b0-a4d5-4e2a-8cf1-516fd1e69421",
   "metadata": {},
   "source": [
    "# Investigate data\n",
    "Here, I investigate the data to find a representative weather station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a48051-a271-4fbd-a393-bbe88e0347cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "api_key = '' # insert your own key between the '' signs\n",
    "DMI_URL = 'https://dmigw.govcloud.dk/v2/metObs/collections/observation/items'\n",
    "r = requests.get(DMI_URL, params={'api-key': api_key}) # Issues a HTTP GET request\n",
    "\n",
    "\n",
    "\n",
    "# Specify one or more station IDs or all_stations\n",
    "stationId = '06109' #'06072' #Ødum\n",
    "stationIds = [stationId]  \n",
    "\n",
    "#Silstrup 06019: Har det hele, men ligger i vestjylland\n",
    "#Isenvad 06068: Har det hele og ligger centralt\n",
    "\n",
    "# Specify one or more parameter IDs or all_parameters\n",
    "parameterIds = ['temp_mean_past1h', 'precip_past1h'] # 'radia_glob_past1h', 'wind_speed_past1h',', 'sun_last1h_glob'\n",
    "\n",
    "# Derive datetime specifier string\n",
    "datetime_str = start_time.tz_localize('UTC').isoformat() + '/' + end_time.tz_localize('UTC').isoformat()\n",
    "\n",
    "dfs, dfi = [], []\n",
    "for station in stationIds:\n",
    "    for parameter in parameterIds:\n",
    "        # Specify query parameters\n",
    "        params = {\n",
    "            'api-key' : api_key,\n",
    "            'datetime' : datetime_str,\n",
    "            'stationId' : station,\n",
    "            'parameterId' : parameter,\n",
    "            'limit' : '300000',  # max limit\n",
    "        }\n",
    "\n",
    "        # Submit GET request with url and parameters\n",
    "        r = requests.get(DMI_URL, params=params)\n",
    "        # Extract JSON object\n",
    "        json = r.json() # Extract JSON object\n",
    "        # Convert JSON object to a MultiIndex DataFrame and add to list\n",
    "        dfi = pd.json_normalize(json['features'])\n",
    "        if dfi.empty is False:\n",
    "            dfi['time'] = pd.to_datetime(dfi['properties.observed'])\n",
    "            \n",
    "            # Drop other columns\n",
    "            dfi = dfi[['time', 'properties.value', 'properties.stationId', 'properties.parameterId']]\n",
    "            \n",
    "            # Rename columns, e.g., 'properties.stationId' becomes 'stationId'\n",
    "            dfi.columns = [c.replace('properties.', '') for c in dfi.columns]\n",
    "            \n",
    "            # Drop identical rows (considers both value and time stamp)\n",
    "            dfi = dfi[~dfi.duplicated()]\n",
    "            dfi = dfi.set_index(['time','stationId','parameterId'])\n",
    "            dfi = dfi['value'].unstack(['stationId','parameterId'])\n",
    "            dfs.append(dfi)\n",
    "\n",
    "data_dmi = pd.concat(dfs, axis='columns').sort_index()\n",
    "print(data_dmi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3eb860-027d-449f-915f-517057668ed7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#stationIds = ['06052', '06132']\n",
    "\n",
    "data_dmi_filter = data_dmi.copy()\n",
    "data_dmi_filter = data_dmi_filter[stationIds]\n",
    "data_dmi_filter.columns = data_dmi_filter.columns.droplevel(0)\n",
    "\n",
    "# Convert the 'time' column timezone without setting it to None explicitly, assuming 'time' is the index or a column after reset_index()\n",
    "data_dmi_filter['time'] = data_dmi_filter.index #['time'].dt.tz_localize(None)\n",
    "data_dmi_filter['time'] = data_dmi_filter['time'].dt.tz_localize(None)\n",
    "data_dmi_filter = data_dmi_filter.set_index('time')\n",
    "print(data_dmi_filter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfaacc0-566c-4f4c-8ee0-49fc56309a89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_date = start_time.strftime('%Y-%m-%d') #2015-01-01'\n",
    "end_date = end_time.strftime('%Y-%m-%d') #'2022-06-01'\n",
    "\n",
    "price_area  = '{\"PriceArea\":[\"DK1\"]}'\n",
    "url = f'https://api.energidataservice.dk/dataset/Elspotprices?start={start_date}&end={end_date}&filter={price_area}'\n",
    "response = requests.get(\n",
    "    url=f'https://api.energidataservice.dk/dataset/Elspotprices?start={start_date}&end={end_date}&filter={price_area}')\n",
    "\n",
    "if response.ok:  # More idiomatic way to check for a successful request\n",
    "    records = response.json().get('records', [])\n",
    "    # Directly filtering necessary columns and renaming them\n",
    "    data_el_spot_DK1 = (pd.json_normalize(records)\n",
    "                          .loc[:, ['HourUTC', 'SpotPriceDKK']]\n",
    "                          .rename(columns={'HourUTC': 'time', 'SpotPriceDKK': 'SpotPriceDK1'}))\n",
    "\n",
    "    # Convert 'time' column to datetime without timezone information\n",
    "    data_el_spot_DK1['time'] = pd.to_datetime(data_el_spot_DK1['time']).dt.tz_localize(None)\n",
    "\n",
    "    # Display the first few rows of the processed DataFrame\n",
    "    print(data_el_spot_DK1)\n",
    "else:\n",
    "    print(f\"Failed to fetch data: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66beee0a-4e13-4141-aff2-e7e6ee9de0d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "price_area  = '{\"PriceArea\":[\"DK2\"]}'\n",
    "\n",
    "response = requests.get(\n",
    "    url=f'https://api.energidataservice.dk/dataset/Elspotprices?start={start_date}&end={end_date}&filter={price_area}')\n",
    "\n",
    "if response.ok:  # More idiomatic way to check for a successful request\n",
    "    records = response.json().get('records', [])\n",
    "    # Directly filtering necessary columns and renaming them\n",
    "    data_el_spot_DK2 = (pd.json_normalize(records)\n",
    "                          .loc[:, ['HourUTC', 'SpotPriceDKK']]\n",
    "                          .rename(columns={'HourUTC': 'time', 'SpotPriceDKK': 'SpotPriceDK2'}))\n",
    "\n",
    "    # Convert 'time' column to datetime without timezone information\n",
    "    data_el_spot_DK2['time'] = pd.to_datetime(data_el_spot_DK2['time']).dt.tz_localize(None)\n",
    "\n",
    "    # Display the first few rows of the processed DataFrame\n",
    "    print(data_el_spot_DK2)\n",
    "else:\n",
    "    print(f\"Failed to fetch data: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c41e1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Definer variable for start, slut, og prisområde\n",
    "price_area = \"DK1\"\n",
    "\n",
    "# Sammensæt URL med de angivne parametre\n",
    "url = f\"https://api.energidataservice.dk/dataset/Forecasts_Hour?offset=0&start={start_date}&end={end_date}&filter=%7B%22PriceArea%22:%5B%22{price_area}%22%5D%7D&sort=HourUTC%20ASC\"\n",
    "\n",
    "# Send an HTTP GET request to the API\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Convert the response data from JSON format to a Python dictionary\n",
    "    data = response.json()\n",
    "\n",
    "    # Load data into a pandas DataFrame assuming 'records' is the key containing the actual data\n",
    "    if 'records' in data:\n",
    "        df = pd.DataFrame(data['records'])\n",
    "\n",
    "        # Pivot the DataFrame to have columns for each forecast type with corresponding values\n",
    "        df_pivoted = df.pivot(index='HourUTC', columns='ForecastType', values='ForecastDayAhead')\n",
    "\n",
    "        print(\"Data loaded and pivoted into DataFrame successfully:\")\n",
    "        print(df_pivoted.head(20))  # Display the first few rows of the pivoted DataFrame\n",
    "    else:\n",
    "        print(\"No 'records' key in JSON response\")\n",
    "else:\n",
    "    print(\"Failed to retrieve data. Status code:\", response.status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedb0e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_weather = df_pivoted.copy().reset_index().rename(columns={'HourUTC': 'time'})\n",
    "forecast_weather['time'] = pd.to_datetime(forecast_weather['time']).dt.tz_localize(None)\n",
    "merged_elspot_weather_forecast = pd.merge(data_el_spot_DK1, forecast_weather, on='time', how='left').set_index('time').interpolate(method='time')\n",
    "print(merged_elspot_weather_forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2abf958",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_values = merged_elspot_weather_forecast[merged_elspot_weather_forecast.isna()]\n",
    "\n",
    "# For at se rækkerne, hvor der findes NaN-værdier, kan du gøre følgende\n",
    "rows_with_nans = merged_elspot_weather_forecast[merged_elspot_weather_forecast.isna().any(axis=1)]\n",
    "\n",
    "# Vis de fundne rækker med NaN-værdier\n",
    "print(rows_with_nans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b1b3d3-50ea-41b5-922f-8d81f9dbcab0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_el_spot_DK1['time'] = pd.to_datetime(data_el_spot_DK1.loc[:,'time'])\n",
    "data_el_spot_DK2['time'] = pd.to_datetime(data_el_spot_DK2.loc[:,'time'])\n",
    "\n",
    "merged_elspot = pd.merge(data_el_spot_DK1, data_el_spot_DK2, on='time', how='inner').set_index('time')\n",
    "print(merged_elspot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eec3a67-b1a4-4103-a467-db79adca5c85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "colors = ['#1E90FF', 'yellow']\n",
    "\n",
    "# Assuming 'merged_elspot' is your DataFrame and 'path_to_images' is defined\n",
    "ax = merged_elspot[['SpotPriceDK1', 'SpotPriceDK2']].plot(\n",
    "    figsize=(8, 5), \n",
    "    legend=False, \n",
    "    fontsize=8, \n",
    "    rot=0, \n",
    "    subplots=True,\n",
    "    color=colors,\n",
    "    grid=True,\n",
    "    xlabel=''   # List of colors for each line\n",
    "    )\n",
    "\n",
    "# Setting labels for each subplot\n",
    "ax[0].set_ylabel('Day Ahead Spot Prices in DK1\\n(DKK/MWh)', fontsize=8)\n",
    "ax[1].set_ylabel('Day Ahead Spot Prices in DK2\\n(DKK/MWh)', fontsize=8)  # Label for the second subplot\n",
    "\n",
    "# Save the plot to a file\n",
    "plt.savefig(f\"{path_to_images}elspot_prices.png\", dpi=150)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe45450-8a29-4823-8baf-397a6de77941",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "merged_data = pd.merge(merged_elspot_weather_forecast.reset_index(), data_dmi_filter, on='time', how='right') \\\n",
    "                .set_index('time') \n",
    "print(merged_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fcd5bedc-33eb-433d-85c8-b160eab69073",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#merged_data = merged_data.dropna()\n",
    "merged_data.to_csv('data/elspot_and_weather_data_hourly.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1713e189-83da-4257-beeb-5d54c6ab84fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "daily_data = merged_data.resample('D').mean() \n",
    "daily_data.to_csv('data/elspot_and_weather_data_daily.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7dff75a-27b5-455c-8d15-c2ca97bec870",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_water = pd.read_excel('data/hydro_reservoir_data.xlsx') #.set_index('Uge')\n",
    "\n",
    "# Start by melting the original DataFrame\n",
    "df_long = pd.melt(df_water, id_vars=['Uge'], var_name='Year', value_name='hydro_reservoir')\n",
    "\n",
    "# Directly convert 'Year' and 'Uge' to a datetime format representing Monday of each week\n",
    "# This combines the conversion to string, zero-padding, and datetime conversion in one step\n",
    "df_long['time'] = pd.to_datetime(df_long['Year'].astype(str) + 'W' + df_long['Uge'].astype(str).str.zfill(2) + '-1', format='%GW%V-%u')\n",
    "\n",
    "# Sort, drop unnecessary columns, and set 'Date' as index in one chained command\n",
    "df_water_final = df_long.sort_values(by='time', ascending=True).drop(columns=['Uge', 'Year']).dropna()\n",
    "print(df_water_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7147f81-e54f-4a67-8ccc-df705424d0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the number of hours in a week\n",
    "hours_in_week = 7 * 24\n",
    "\n",
    "# Create a new DataFrame to hold the hourly data\n",
    "# This involves repeating each weekly row 168 times (for each hour of the week)\n",
    "# and then dividing the 'Level' by 168 to distribute it evenly\n",
    "hourly_water_data = df_water_final.reindex(df_water_final.index.repeat(hours_in_week))\n",
    "\n",
    "# Assuming 'Level' is the column with weekly data to be distributed\n",
    "#hourly_water_data['hydro_reservoir'] /= hours_in_week\n",
    "hourly_water_data = hourly_water_data.set_index('time')\n",
    "\n",
    "# Generate an hourly time range that matches the length of the new DataFrame\n",
    "hourly_range = pd.date_range(start=hourly_water_data.index[0], periods=len(hourly_water_data), freq='h')\n",
    "\n",
    "# Assign this hourly range as the new index\n",
    "hourly_water_data['time'] = hourly_range #.sort_index(descending=True)\n",
    "hourly_water_data = hourly_water_data.reset_index(drop=True).sort_values(by='time', ascending=True)\n",
    "print(hourly_water_data)\n",
    "\n",
    "#hourly_water_data['time'] = pd.to_datetime(df_water_final['time'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338bf3bf-38f2-456a-9b9b-c14452d43606",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data_updated = pd.merge(merged_data, hourly_water_data, on = 'time', how = 'left')\n",
    "\n",
    "# Reverses order of index to get earliest date first\n",
    "merged_data_updated = merged_data_updated.iloc[::-1].reset_index(drop=False)\n",
    "merged_data_updated = merged_data_updated.set_index('time').drop(columns=['index']).interpolate(method='time').dropna()\n",
    "print(merged_data_updated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c516f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hourly_water_data.set_index('time', inplace=True)  # Setting time as index if not already set\n",
    "\n",
    "plot = pd.DataFrame(merged_data_updated['hydro_reservoir'])\n",
    "\n",
    "# Creating a figure with dynamic subplots based on the number of columns\n",
    "fig, axes = plt.subplots(nrows=len(plot.columns), ncols=1, figsize=(12, 5 * len(plot.columns)), sharex=True)\n",
    "\n",
    "# Check if there's only one column and ensure 'axes' is iterable\n",
    "if plot.columns.size == 1:\n",
    "    axes = [axes]  # Make a list of axes if only one plot\n",
    "\n",
    "# Loop through each column and create a bar plot on its respective subplot\n",
    "for ax, column in zip(axes, plot.columns):\n",
    "    ax.bar(plot.index, plot[column], width=0.8, label=column, color='skyblue')  # Adjust width as necessary\n",
    "    ax.set_title(column)\n",
    "    ax.set_ylabel('Values')\n",
    "    ax.grid(True)\n",
    "    ax.legend(loc='upper right')\n",
    "\n",
    "# Formatting the x-axis to handle date labels better\n",
    "plt.gcf().autofmt_xdate()  # Auto-format date labels for better readability\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fff7f8",
   "metadata": {},
   "source": [
    "## Importing Carbon, Coal and Natural Gas prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860c711f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_carbon = pd.read_csv('data/European Union Allowance (EUA) Yearly Futures Historical Data.csv', usecols=['Date', 'Price']).dropna()\n",
    "df_carbon.rename(columns={'Price': 'carbon_price', 'Date': 'time'}, inplace=True)\n",
    "df_carbon['time'] = pd.to_datetime(df_carbon['time'])\n",
    "df_carbon.set_index('time', inplace=True)\n",
    "\n",
    "# Load and prepare coal data\n",
    "df_coal = pd.read_csv('data/Coal_09_06_24-12_10_18.csv', usecols=['Date', 'Close']).dropna()\n",
    "df_coal.rename(columns={'Close': 'coal_price', 'Date': 'time'}, inplace=True)\n",
    "df_coal['time'] = pd.to_datetime(df_coal['time'])\n",
    "df_coal.set_index('time', inplace=True)\n",
    "\n",
    "# Load and prepare natural gas data\n",
    "df_natural_gas = pd.read_csv('data/Natural Gas (Henry Hub)_09_06_24-12_10_18.csv', usecols=['Date', 'Close']).dropna()\n",
    "df_natural_gas.rename(columns={'Close': 'natural_gas_price', 'Date': 'time'}, inplace=True)\n",
    "df_natural_gas['time'] = pd.to_datetime(df_natural_gas['time'])\n",
    "df_natural_gas.set_index('time', inplace=True)\n",
    "\n",
    "# Merge all datasets\n",
    "df_commodity_merged = pd.merge(df_coal, df_carbon, left_index=True, right_index=True, how='right')\n",
    "df_commodity_merged = pd.merge(df_commodity_merged, df_natural_gas, left_index=True, right_index=True, how='left')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8634e16a",
   "metadata": {},
   "source": [
    "## Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2b19de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(df_commodity_merged)\n",
    "\n",
    "# Upsample to hourly data, using forward fill to carry the last valid observation forward\n",
    "df_hourly = df.resample('h').ffill()\n",
    "\n",
    "# Now df_hourly contains the upsampled data, but it's constant throughout each day.\n",
    "# To create a linear transition between days, use .interpolate()\n",
    "df_commodity_merged_final = df_hourly.interpolate(method='time')\n",
    "print(df_commodity_merged_final)\n",
    "#df_carbon_final = df_hourly.drop('date', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727d347b",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data_full = pd.merge(merged_data_updated, df_commodity_merged_final, on = 'time', how = 'left')\n",
    "print(merged_data_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ace6e9b",
   "metadata": {},
   "source": [
    "# Illustrate data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79246c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data_full_inter = merged_data_full.interpolate(method='time')\n",
    "\n",
    "merged_data_full_inter.to_csv('data/el_weather_hydro_coal_carbon_data.csv', index=True)\n",
    "print(merged_data_full_inter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7870298",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.dates as mdates\n",
    "\n",
    "names = {\n",
    "    'SpotPriceDK1': ['Day Ahead Spot Price', '#1E90FF'],\n",
    "    'Offshore Wind': ['Offshore Wind', 'skyblue'],\n",
    "    'Onshore Wind': ['Onshore Wind', 'aquamarine'],\n",
    "    'Solar': ['Solar', 'yellow'],\n",
    "    'precip_past1h': ['Precipitation', 'lightskyblue'],\n",
    "    'temp_mean_past1h': ['Temperature', 'red'],\n",
    "    'hydro_reservoir': ['Hydro Reservoir Levels', 'skyblue'],\n",
    "    'carbon_price': ['CO2 Price (Index)', 'lightgreen'],\n",
    "    'coal_price': ['Coal Price', '#696969'],\n",
    "    'natural_gas_price': ['Natural Gas Price', 'yellowgreen']\n",
    "}\n",
    "\n",
    "params_to_plot = list(names.keys())\n",
    "end_time_plot = end_time\n",
    "\n",
    "# Ensuring the 'time' column is a datetime object\n",
    "data_to_plot = merged_data_full_inter.reset_index() #.interpolate(method='time')\n",
    "data_to_plot['time'] = pd.to_datetime(data_to_plot['time'])\n",
    "data_to_plot = data_to_plot[data_to_plot['time'] <= end_time_plot]\n",
    "\n",
    "# Create a complete datetime index from min to max time, with a specified frequency if necessary\n",
    "full_time_index = pd.date_range(start=data_to_plot['time'].min(), end=end_time_plot, freq='h')  # Adjust the frequency 'H' as needed\n",
    "\n",
    "# Reindex the DataFrame to this full index\n",
    "data_to_plot.set_index('time', inplace=True)\n",
    "data_to_plot = data_to_plot.reindex(full_time_index).interpolate(method='time')\n",
    "#final_dataset = data_to_plot.copy()\n",
    "\n",
    "# Create a figure with dynamic subplots based on the number of parameters\n",
    "fig, axes = plt.subplots(len(params_to_plot), 1, figsize=(12, 2 * len(params_to_plot)), sharex=True)\n",
    "\n",
    "# If there's only one parameter, `axes` won't be a list, so we need to make it iterable\n",
    "if len(params_to_plot) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "# Loop through each parameter and corresponding axis\n",
    "for i, par in enumerate(params_to_plot):\n",
    "    \n",
    "    if par in ['Onshore Wind', 'Offshore Wind', 'Solar', 'precip', 'temp_mean', 'hydro_reservoir']:\n",
    "        axes[i].bar(data_to_plot.index, data_to_plot[par], color=names[par][1])\n",
    "    else:\n",
    "        axes[i].plot(data_to_plot.index, data_to_plot[par], color=names[par][1])\n",
    "    \n",
    "    axes[i].set_ylabel(names[par][0])\n",
    "    axes[i].tick_params(axis='y')\n",
    "    axes[i].grid(True)\n",
    "\n",
    "# Set x-axis limits explicitly using pd.Timestamp on the last subplot\n",
    "axes[-1].set_xlim(pd.Timestamp(start_time), pd.Timestamp(end_time_plot))\n",
    "\n",
    "# Use YearLocator and DateFormatter to format the x-axis on the last subplot\n",
    "axes[-1].xaxis.set_major_locator(mdates.YearLocator())\n",
    "axes[-1].xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "\n",
    "# Rotate x-ticks for better readability on the last subplot\n",
    "plt.savefig(f\"{path_to_images}wheater_variables_and_hydro_coal_carbon_final_with_bars.png\", dpi=150)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4fc3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mdates\n",
    "\n",
    "names = {\n",
    "    #'SpotPriceDK1': ['Day Ahead Spot Price', '#1E90FF'],\n",
    "    'Offshore Wind': ['Offshore Wind\\nproduction (MWh)', 'skyblue'],\n",
    "    'Onshore Wind': ['Onshore Wind\\nproduction (MWh)', 'aquamarine'],\n",
    "    'Solar': ['Solar production\\n(MWh)', 'yellow'],\n",
    "    'precip_past1h': ['Precipitation\\n(mm)', 'lightskyblue'],\n",
    "    'temp_mean_past1h': ['Temperature\\n(°C)', 'red'],\n",
    "    'hydro_reservoir': ['Hydro Reservoir Levels\\n(%)', 'skyblue'],\n",
    "    'carbon_price': ['CO2 Price\\n(Index)', 'lightgreen'],\n",
    "    'coal_price': ['Coal Price\\n(USD/Ton)', '#696969'],\n",
    "    'natural_gas_price': ['Natural Gas Price\\n(USD/MMBtu)', 'yellowgreen']\n",
    "}\n",
    "\n",
    "params_to_plot = list(names.keys())\n",
    "end_time_plot = end_time\n",
    "\n",
    "# Ensuring the 'time' column is a datetime object\n",
    "data_to_plot = merged_data_full_inter.reset_index() #.interpolate(method='time')\n",
    "data_to_plot['time'] = pd.to_datetime(data_to_plot['time'])\n",
    "data_to_plot = data_to_plot[data_to_plot['time'] <= end_time_plot]\n",
    "\n",
    "# Create a complete datetime index from min to max time, with a specified frequency if necessary\n",
    "full_time_index = pd.date_range(start=data_to_plot['time'].min(), end=end_time_plot, freq='h')  # Adjust the frequency 'H' as needed\n",
    "\n",
    "# Reindex the DataFrame to this full index\n",
    "data_to_plot.set_index('time', inplace=True)\n",
    "data_to_plot = data_to_plot.reindex(full_time_index).interpolate(method='time')\n",
    "#final_dataset = data_to_plot.copy()\n",
    "\n",
    "# Create a figure with dynamic subplots based on the number of parameters\n",
    "fig, axes = plt.subplots(len(params_to_plot), 1, figsize=(12, 2 * len(params_to_plot)), sharex=True)\n",
    "\n",
    "# If there's only one parameter, `axes` won't be a list, so we need to make it iterable\n",
    "if len(params_to_plot) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "# Loop through each parameter and corresponding axis\n",
    "for i, par in enumerate(params_to_plot):\n",
    "    \n",
    "    if par in ['Onshore Wind', 'Offshore Wind', 'Solar', 'precip', 'temp_mean', 'hydro_reservoir']:\n",
    "        axes[i].bar(data_to_plot.index, data_to_plot[par], color=names[par][1])\n",
    "    else:\n",
    "        axes[i].plot(data_to_plot.index, data_to_plot[par], color=names[par][1])\n",
    "    \n",
    "    axes[i].set_ylabel(names[par][0])\n",
    "    axes[i].tick_params(axis='y')\n",
    "    axes[i].grid(True)\n",
    "\n",
    "# Set x-axis limits explicitly using pd.Timestamp on the last subplot\n",
    "axes[-1].set_xlim(pd.Timestamp(start_time), pd.Timestamp(end_time_plot))\n",
    "\n",
    "# Use YearLocator and DateFormatter to format the x-axis on the last subplot\n",
    "axes[-1].xaxis.set_major_locator(mdates.YearLocator())\n",
    "axes[-1].xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "\n",
    "# Rotate x-ticks for better readability on the last subplot\n",
    "plt.savefig(f\"{path_to_images}exogenous_variables.png\", dpi=150)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778b5d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = {\n",
    "    #'SpotPriceDK1': ['Day Ahead Spot Price', '#1E90FF'],\n",
    "    'Offshore Wind': ['Offshore Wind\\nproduction (MWh)', 'skyblue'],\n",
    "    'Onshore Wind': ['Onshore Wind\\nproduction (MWh)', 'aquamarine'],\n",
    "    'Solar': ['Solar production\\n(MWh)', 'yellow'],\n",
    "    'precip_past1h': ['Precipitation\\n(mm)', 'lightskyblue'],\n",
    "    'temp_mean_past1h': ['Temperature\\n(°C)', 'red'],\n",
    "    'hydro_reservoir': ['Hydro Reservoir Levels\\n(%)', 'skyblue'],\n",
    "    'carbon_price': ['CO2 Price\\n(Index)', 'lightgreen'],\n",
    "    'coal_price': ['Coal Price\\n(USD/Ton)', '#696969'],\n",
    "    'natural_gas_price': ['Natural Gas Price\\n(USD/MMBtu)', 'yellowgreen']\n",
    "}\n",
    "\n",
    "params_to_plot = list(names.keys())\n",
    "end_time_plot = end_time\n",
    "\n",
    "# Ensuring the 'time' column is a datetime object\n",
    "data_to_plot = merged_data_full_inter.reset_index() #.interpolate(method='time')\n",
    "data_to_plot['time'] = pd.to_datetime(data_to_plot['time'])\n",
    "data_to_plot = data_to_plot[data_to_plot['time'] <= end_time_plot]\n",
    "\n",
    "# Create a complete datetime index from min to max time, with a specified frequency if necessary\n",
    "full_time_index = pd.date_range(start=data_to_plot['time'].min(), end=end_time_plot, freq='h')  # Adjust the frequency 'H' as needed\n",
    "\n",
    "# Reindex the DataFrame to this full index\n",
    "data_to_plot.set_index('time', inplace=True)\n",
    "data_to_plot = data_to_plot.reindex(full_time_index).interpolate(method='time')\n",
    "#final_dataset = data_to_plot.copy()\n",
    "\n",
    "\n",
    "names = {\n",
    "    'SpotPriceDK1': ['Day Ahead Spot Price', '#1E90FF'],\n",
    "    'Offshore Wind': ['Offshore Wind', 'skyblue'],\n",
    "    'Onshore Wind': ['Onshore Wind', 'aquamarine'],\n",
    "    'Solar': ['Solar', 'yellow'],\n",
    "    'precip_past1h': ['Precipitation', 'lightskyblue'],\n",
    "    'temp_mean_past1h': ['Temperature', 'red'],\n",
    "    'hydro_reservoir': ['Hydro Reservoir Levels', 'skyblue'],\n",
    "    'carbon_price': ['CO2 Price', 'lightgreen'],\n",
    "    'coal_price': ['Coal Price', '#696969'],\n",
    "    'natural_gas_price': ['Natural Gas Price', 'yellowgreen']\n",
    "}\n",
    "\n",
    "# Rename the columns of final_dataset\n",
    "final_dataset = data_to_plot.copy()\n",
    "\n",
    "# Create a copy and rename the columns in one step\n",
    "final_dataset = data_to_plot.rename(columns={old_name: new_name[0] for old_name, new_name in names.items()}).reset_index()\n",
    "\n",
    "# Rename the 'index' column to 'time' directly while resetting the index\n",
    "final_dataset.rename(columns={'index': 'time'}, inplace=True)\n",
    "\n",
    "# Save the updated DataFrame to a CSV file\n",
    "final_dataset.to_csv('data/dataset.csv', index=False)  # Use index=False since 'time' is now a column\n",
    "\n",
    "# Print the first few rows to verify the changes\n",
    "print(final_dataset.head())\n",
    "print(final_dataset.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3064850",
   "metadata": {},
   "source": [
    "# Dummies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03eada47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Antager at 'final_dataset' er dit datasæt\n",
    "df = pd.DataFrame(final_dataset)\n",
    "\n",
    "# Kontrollerer kolonnerne i DataFrame\n",
    "\n",
    "\n",
    "# Reset index\n",
    "df.reset_index(inplace=True)\n",
    "\n",
    "# Konverterer 'time' kolonnen til datetime (opdater navnet hvis nødvendigt)\n",
    "df['time'] = pd.to_datetime(df['time'])  # Sørg for at 'time' er korrekt\n",
    "\n",
    "# Opretter dummyvariabler for ugedage\n",
    "df = pd.get_dummies(df, columns=['time'], drop_first=False)\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc94c1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Antager, at 'final_dataset' er dit datasæt\n",
    "df = pd.DataFrame(final_dataset.copy())\n",
    "\n",
    "# Reset index\n",
    "df.reset_index(inplace=True)\n",
    "\n",
    "# Konverterer 'time' kolonnen til datetime (sørg for navnet er korrekt)\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "# Opretter en ny kolonne for ugedagen\n",
    "df['weekday'] = df['time'].dt.day_name()\n",
    "\n",
    "# Oprettelse af dummyvariabler for hver dag i ugen\n",
    "df = pd.get_dummies(df, columns=['weekday'], drop_first=False)\n",
    "\n",
    "#df.set_index('time', inplace=True)\n",
    "df.drop(columns=['index'], inplace=True)\n",
    "# Viser resultatet\n",
    "# Save the updated DataFrame to a CSV file\n",
    "df.to_csv('data/dataset_with_dummies.csv', index=False) \n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bd10cb",
   "metadata": {},
   "source": [
    "# Lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c13e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lags = df.copy()\n",
    "\n",
    "# Variables to lag by 24 hours\n",
    "lags_names = [\n",
    "    'Precipitation', \n",
    "    'Temperature',\n",
    "    'Hydro Reservoir Levels',\n",
    "    'CO2 Price',\n",
    "    'Coal Price',\n",
    "    'Natural Gas Price'\n",
    "]\n",
    "\n",
    "# Lag each specified column by one day and replace the original\n",
    "for column in lags_names:\n",
    "    df_lags[column] = df_lags[column].shift(24)\n",
    "\n",
    "# Variables to lag specified hours\n",
    "lags_names = [\n",
    "    'Day Ahead Spot Price',\n",
    "]\n",
    "\n",
    "# Lag each specified column by one day\n",
    "for column in lags_names:\n",
    "    df_lags[column + '_lagged_1_hour'] = df_lags[column].shift(1)\n",
    "\n",
    "# Lag each specified column by one day\n",
    "for column in lags_names:\n",
    "    df_lags[column + '_lagged_2_hour'] = df_lags[column].shift(2)\n",
    "\n",
    "\n",
    "df_lags.dropna(inplace=True)\n",
    "\n",
    "# Display the DataFrame with lagged variables\n",
    "print(df_lags.head())\n",
    "\n",
    "df_lags.to_csv('data/dataset_with_dummies_and_lags.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9496a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lags = df_lags.copy()\n",
    "\n",
    "# Variables to lag\n",
    "lags_names = [\n",
    "    'Day Ahead Spot Price',\n",
    "]\n",
    "df_lags = df.copy()\n",
    "\n",
    "# Lag each specified column by one day\n",
    "for column in lags_names:\n",
    "    df_lags[column + '_lagged_24_hour'] = df_lags[column].shift(24)\n",
    "\n",
    "# Lag each specified column by one day\n",
    "for column in lags_names:\n",
    "    df_lags[column + '_lagged_25_hour'] = df_lags[column].shift(25)\n",
    "\n",
    "df_lags.dropna(inplace=True)\n",
    "# Display the DataFrame with lagged variables\n",
    "print(df_lags.head())\n",
    "\n",
    "df_lags.to_csv('data/final_dataset.csv', index=False) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
